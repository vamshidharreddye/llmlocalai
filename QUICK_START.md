# Archived Quick Start

This quick-start guide has been archived and moved to `archive/removed_docs/QUICK_START.md`.

If you need the original quick start instructions, open the archived copy.

-- Cleaned by maintenance script
# ğŸš€ Quick Start Guide - Local AI Chatbot

## âœ… Everything is Ready!

Your local AI chatbot is now fully set up and running. Here's what's active:

- **âœ… Ollama** - Running at `http://localhost:11434`
- **âœ… Backend** - Running at `http://127.0.0.1:8000`
- **âœ… Frontend** - Running at `http://localhost:3000`

## ğŸ¯ What to Do Now

### 1. **Download an Ollama Model** (If you haven't already)

Open a PowerShell terminal and run:

```powershell
ollama pull mistral
```

Or try a smaller/faster model:
```powershell
ollama pull neural-chat
```

Check available models:
```powershell
ollama list
```

### 2. **Use the Chatbot**

1. Go to **http://localhost:3000** in your browser
2. Type a question and press Enter
3. Ollama will generate an answer (first time takes longer as it loads the model)

### 3. **Example Questions to Try**

- "What is machine learning?"
- "Explain blockchain in simple terms"
- "Write a Python hello world script"
- "What are the steps to make pizza?"
- "Summarize the importance of Python"

## ğŸ“ Project Structure

```
llmlocalai/
â”œâ”€â”€ ğŸŒ http://localhost:3000          â† Open this in browser
â”œâ”€â”€ ğŸ”Œ http://127.0.0.1:8000          â† Backend API
â”œâ”€â”€ ğŸ¤– http://localhost:11434         â† Ollama (local LLM)
â”œâ”€â”€ ğŸ“„ index.html                      â† UI
â”œâ”€â”€ ğŸ“„ server.py                       â† UI Server
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app_simple.py                 â† Simplified backend (currently running)
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data/                             â† Your files go here (for future indexing)
â””â”€â”€ .env                              â† Configuration
```

## âŒ› How It Works

1. **You type**: A question in the chat
2. **Backend**: Receives your question
3. **Ollama**: Generates an intelligent answer locally on your computer
4. **Response**: You get the answer instantly

## ğŸ› ï¸ Stopping Services

To stop a service, go to its terminal window and press **Ctrl+C**:

```powershell
# Stop Ollama (first terminal)
Ctrl+C

# Stop Backend (second terminal)
Ctrl+C

# Stop UI Server (third terminal)
Ctrl+C
```

## ğŸ”„ Restarting Services

### Restart everything:

```powershell
# Terminal 1: Ollama
ollama serve

# Terminal 2: Backend
cd C:\Users\VAMSHI\llmlocalai\backend
python -m uvicorn app_simple:app --reload --host 127.0.0.1 --port 8000

# Terminal 3: UI
cd C:\Users\VAMSHI\llmlocalai
python server.py
```

Or use the automated launcher:

```powershell
.\LAUNCH.ps1
```

## ğŸ“š Available Commands

**Test the backend:**
```powershell
# Check if backend is running
curl http://127.0.0.1:8000/health

# Ask a question via API
$body = @{query="Hello, how are you?"} | ConvertTo-Json
curl -X POST http://127.0.0.1:8000/ask `
  -H "Content-Type: application/json" `
  -d $body
```

**Check Ollama:**
```powershell
# List available models
curl http://localhost:11434/api/tags

# Check Ollama is running
ollama list
```

## ğŸ¨ Features

âœ… **Real-time Chat** - Get instant responses  
âœ… **Multiple Models** - Switch models with `ollama pull MODEL_NAME`  
âœ… **Fully Local** - Nothing leaves your computer  
âœ… **Private** - No cloud, no tracking  
âœ… **Fast** - Runs on your hardware  
âœ… **No Setup** - Just click and chat  

## ğŸ’¡ Tips & Tricks

1. **Try different models:**
   ```powershell
   ollama pull mistral        # Fast, accurate
   ollama pull neural-chat    # Optimized for chat
   ollama pull llama2         # Larger, more powerful
   ```

2. **Set default model in `.env`:**
   ```ini
   OLLAMA_MODEL=mistral
   ```

3. **Be specific with questions** for better answers:
   - âŒ "Tell me about programming"
   - âœ… "Explain Python decorators with examples"

4. **Check system resources:**
   ```powershell
   Get-Process | Where-Object {$_.ProcessName -like "*ollama*" -or $_.Name -like "*python*"} | Format-Table
   ```

## ğŸ› Troubleshooting

**Q: Ollama not responding?**
```powershell
# Check if it's running
Test-NetConnection -ComputerName localhost -Port 11434

# Or check process
Get-Process -Name ollama -ErrorAction SilentlyContinue
```

**Q: Backend not starting?**
```powershell
cd C:\Users\VAMSHI\llmlocalai\backend
python -m uvicorn app_simple:app --host 127.0.0.1 --port 8000
```

**Q: No response from chatbot?**
1. Make sure Ollama has a model: `ollama list`
2. Download one: `ollama pull mistral`
3. Refresh the page
4. Try asking again

**Q: Want to use the full featured version with file indexing?**

The current version (app_simple.py) uses direct LLM queries. The full version (app.py) with file indexing is available but requires fixing dependency issues. You can upgrade later if needed.

## ğŸš€ Next Steps

1. âœ… Chat with Ollama (you're here!)
2. ğŸ“ [Optional] Add file indexing for local document search
3. ğŸ¨ [Optional] Customize the UI
4. ğŸš€ [Optional] Deploy to a web server

## ğŸ“ Quick Reference

| What | Where |
|------|-------|
| Chat UI | http://localhost:3000 |
| API | http://127.0.0.1:8000 |
| Ollama | http://localhost:11434 |
| Config | `.env` file |
| Backend logs | Terminal window |
| Ollama logs | Terminal window |

---

**You're all set! ğŸ‰ Start asking questions at http://localhost:3000**

Enjoy your private, local AI assistant!
